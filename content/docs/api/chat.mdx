---
title: Chat Completions
description: Generate chat completions using any supported AI model
---

# Chat *Completions*

Generate chat completions using any supported AI model via an OpenAI-compatible endpoint.

## Endpoint

```bash
POST https://api.bluesminds.com/v1/chat/completions
```

## Prerequisites

- A valid API key (see [`authentication.mdx`](../authentication.mdx))
- A model id available on your gateway (discover via [`models.mdx`](./models.mdx))

## Request body

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `model` | string | Yes | Model identifier (for example, `gpt-4o` or `claude-sonnet-4.5`) |
| `messages` | array | Yes | Array of message objects (`role`, `content`) |
| `temperature` | number | No | Sampling temperature (typically `0`–`2`) |
| `max_tokens` | number | No | Maximum tokens to generate |
| `stream` | boolean | No | Stream responses as SSE |

## Example request

```typescript
const response = await fetch('https://api.bluesminds.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    Authorization: 'Bearer bm_live_your_api_key',
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'gpt-4o',
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      { role: 'user', content: 'What is the capital of France?' },
    ],
    temperature: 0.7,
    max_tokens: 500,
  }),
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

## Example response

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1677652288,
  "model": "gpt-4o",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "The capital of France is Paris."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 8,
    "total_tokens": 28
  }
}
```

## Supported models

Model availability depends on how your gateway is configured. Use [`GET /v1/models`](./models.mdx) to retrieve the current list of supported model ids.

<Callout>
**Note:** If a model id is not returned by `/v1/models`, requests using that `model` value will fail with a “model not found” error.
</Callout>

## Troubleshooting

### 401 Unauthorized

- Ensure you are sending `Authorization: Bearer <key>`.
- Verify the key is active and belongs to the correct environment (test vs live).

### Model not found

- Call [`/v1/models`](./models.mdx) and use an `id` from the response.
- If self-hosting, confirm providers and model mappings exist in the admin UI.

### Streaming issues

If `stream: true` does not stream in your environment, see [`streaming-chat.mdx`](../guides/common-tasks/streaming-chat.mdx).

## Next steps

- Discover models: [`models.mdx`](./models.mdx)
- Practical workflows: [`guides/index.mdx`](../guides/index.mdx)
